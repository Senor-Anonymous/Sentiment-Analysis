{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMYYnguq9qZ5rJRzSjtcKGx"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bdc8107"
      },
      "source": [
        "%pip install nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4099f72c"
      },
      "source": [
        "# Download necessary NLTK data\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab') # Added to resolve LookupError"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3e7a6676"
      },
      "source": [
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "class TextPreprocessor:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.stemmer = PorterStemmer()\n",
        "        self.lemmatizer = WordNetLemmatizer()\n",
        "        self.stop_words = set(stopwords.words('english'))\n",
        "        # A basic dictionary for contractions, can be expanded\n",
        "        self.contractions_map = {\n",
        "            \"don't\": \"do not\",\n",
        "            \"won't\": \"will not\",\n",
        "            \"can't\": \"cannot\",\n",
        "            \"it's\": \"it is\",\n",
        "            \"i'm\": \"i am\",\n",
        "            \"he's\": \"he is\",\n",
        "            \"she's\": \"she is\",\n",
        "            \"we're\": \"we are\",\n",
        "            \"they're\": \"they are\",\n",
        "            \"you're\": \"you are\",\n",
        "            \"i've\": \"i have\",\n",
        "            \"we've\": \"we have\",\n",
        "            \"you've\": \"you have\",\n",
        "            \"they've\": \"they have\",\n",
        "            \"i'd\": \"i would\",\n",
        "            \"we'd\": \"we would\",\n",
        "            \"you'd\": \"you would\",\n",
        "            \"they'd\": \"they would\",\n",
        "            \"isn't\": \"is not\",\n",
        "            \"aren't\": \"are not\",\n",
        "            \"wasn't\": \"was not\",\n",
        "            \"weren't\": \"were not\",\n",
        "            \"haven't\": \"have not\",\n",
        "            \"hasn't\": \"has not\",\n",
        "            \"hadn't\": \"had not\",\n",
        "            \"wouldn't\": \"would not\",\n",
        "            \"don't\": \"do not\",\n",
        "            \"doesn't\": \"does not\",\n",
        "            \"didn't\": \"did not\",\n",
        "            \"shouldn't\": \"should not\",\n",
        "            \"couldn't\": \"could not\",\n",
        "            \"mustn't\": \"must not\",\n",
        "            \"mightn't\": \"might not\"\n",
        "        }\n",
        "\n",
        "    def expand_contractions(self, text):\n",
        "        def replace(match):\n",
        "            return self.contractions_map.get(match.group(0).lower(), match.group(0))\n",
        "        # Use regex to find contractions and replace them\n",
        "        return re.sub(r\"\\b(?:\" + \"|\".join(re.escape(k) for k in self.contractions_map.keys()) + r\")\\b\", replace, text, flags=re.IGNORECASE)\n",
        "\n",
        "    def remove_special_chars(self, text):\n",
        "        # Keep alphanumeric characters, spaces, and decimal points\n",
        "        return re.sub(r'[^a-zA-Z0-9\\s.]', '', text)\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        # Convert text to lowercase and tokenize\n",
        "        return word_tokenize(text.lower())\n",
        "\n",
        "    def remove_stopwords(self, tokens):\n",
        "        return [word for word in tokens if word not in self.stop_words]\n",
        "\n",
        "    def stem(self, tokens):\n",
        "        return [self.stemmer.stem(word) for word in tokens]\n",
        "\n",
        "    def lemmatize(self, tokens):\n",
        "        return [self.lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "    def preprocess(self, text, use_lemmatization=False):\n",
        "        text = self.expand_contractions(text)\n",
        "        text = self.remove_special_chars(text)\n",
        "        tokens = self.tokenize(text)\n",
        "        tokens = self.remove_stopwords(tokens)\n",
        "        if use_lemmatization:\n",
        "            tokens = self.lemmatize(tokens)\n",
        "        else:\n",
        "            tokens = self.stem(tokens)\n",
        "\n",
        "        # Correctly remove trailing periods by creating a new list\n",
        "        processed_tokens = []\n",
        "        for token in tokens:\n",
        "            if token and token[-1] == \".\": # Check if token is not empty and has a trailing period\n",
        "                processed_tokens.append(token[0:-1])\n",
        "            else:\n",
        "                processed_tokens.append(token)\n",
        "        for i in processed_tokens:\n",
        "          if i==\" \" or i==\"\":\n",
        "              processed_tokens.remove(i)\n",
        "        return processed_tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessor = TextPreprocessor()\n",
        "c = preprocessor.preprocess(\"Apple Inc. shares fell 2.5% amid concerns about iPhone 15 demand.\", use_lemmatization=True)\n",
        "print(c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EBkO3sKzEYWk",
        "outputId": "8ee669f3-a924-46a9-ae07-41e7b62beba7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['apple', 'inc', 'share', 'fell', '2.5', 'amid', 'concern', 'iphone', '15', 'demand']\n"
          ]
        }
      ]
    }
  ]
}